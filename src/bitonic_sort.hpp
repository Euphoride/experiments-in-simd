/**
 * @brief   Bitonic sort! Might thread this as the target for a mergesort routine later.
 *
 * @date    2025-08-05
 * @version 0.1
 *
 * @details
 *  - dummy data is autogenerated for benchmarking purposes, we'll microbenchmark over
 *    a range of uniformly generated numbers for the model to remove data-dependent
 *    artifacts in the benchmark.
 *  - using apple silicon (an M3 specifically) so i'll be using NEON intrinsics rather
 *    than AVX or similar.
 *  - ...
 *
 * @license MIT
 */

#pragma once

#include <cstddef>

namespace simd {

/**
 * @brief   Sorts a small array in-place using a bitonic sort network (SIMD-enabled)
 *
 * @param   data     pointer to float array to sort
 * @param   length   number of elements (power of two, <=16)
 *
 * @details
 * Uses NEON intrinsics to perform compare-and-swap operations.
 */
void bitonic_sort_optimised(float* data, std::size_t length);

/**
 * @brief   Sorts a small array in-place using a bitonic sort network.
 *
 * @param   data     pointer to float array to sort
 * @param   length   number of elements (power of two, <=16)
 *
 * @details
 * The naive, slow implementation :)
 */
void bitonic_sort_unoptimised(float* data, std::size_t length);

/* ==>> There's a world here where we also compare to std::sort() <<== */

}  // namespace simd
